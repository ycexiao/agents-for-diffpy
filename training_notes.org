Human:
Initialization: (Load structure, profile. Setup Qmax, Qmin, ...)
Rw = bad enough
threshould = good enough
iter_max = my patience
i = 0
While Rw > threshould and  i < iter_max:
    Refine i-th set of variables
    update Rw
    i += 1
    
Descision made by human in this process:
1. the value of threshould and iter_max
2. the sequence of variable refinement: ((a1, a2), (a6, a3, a1), (a4), ...)


Agent:
Initialization: (state space, action space, environment)
value of all state / state-action = 0
policy = naive starting policy
threshould = good enough
iter_max = computation resources
i=0

1. Execute the current policy and generate (state, action, reward) trajectory
2. Calculate initial value function according to the trajectory
for i < iter_max:
    Explore / Exploit the policy and generate SAR trajectory according to the value function and the interaction with environment
    update value function SAR
    
policy creates SAR. Value function can be estimated, updated by SAR. policy maximized/minized by value function.
   

define value-iteration(value function, policy):
    while current-state != terminate state and i < iter_max:
        current-action = policy(current-state)
	next-state, next-reward = Interact_with_env(current-state, current-action)
	backup current-state, next-reward
	current-state = next-state

	when appropriate to update value function:
	    update value function according to the current backups

    return updated value function

define policy-iteration(value function, policy):
    while current-state != terminate state and i < iter_max:
        old-action = policy(current-state)
	i-th state, i-th reward = interact environment with (i-1)-th state and (i-1)-th action
    



State: materials model, materials model parameters and experimental data
Action: refinement with (variables)
Reward: The Rw for the current parameters.


From Chat-gpt:

Treat the problem as an episodic deterministic MDP where each episode = one full candidate sequence.

If actions are continuous use off-policy actor-critic (TD3 / SAC) or on-policy (PPO) if you want stability. If discrete, use DQN / C51 / Rainbow (or PPO for discrete).

Because the reward is likely sparse (only known after full sequence), use a surrogate model (model-based / value shaping) to provide dense guidance and greatly speed learning.

Combine model-free RL (to learn robust policies) with model-based planning (MPC / rollout with surrogate) and curriculum / shaping to handle sparsity.


State : the partial sequence built so far (e.g., concatenated decision tokens, or current vector plus history). Include any useful features (position index, remaining steps).

Action : next decision (discrete choice or continuous component).

Transition: deterministic update s_{t+1} = append(s_t, a_t).

Reward : ideally the final objective r_T = f(sequence). To avoid very sparse learning, give shaped reward r_t = surrogate.predict(s_t) or Δ surrogate (see below).

Episode length: number of sequential decisions needed.


Two practical architectures (pick one)
A — Model-free + surrogate shaping (recommended start)

Learn a surrogate model 
f^(partial sequence)
(partial sequence) (neural net or Gaussian Process) that predicts final 
f from partial sequences.

Use PPO or SAC / TD3 as the RL algorithm. At each step give reward = change in surrogate prediction r_t = \hat f(s_{t+1}) - \hat f(s_t) (dense shaping). At episode end compute true f and add that to buffer and use it to update surrogate.

Benefits: stable policy learning with dense signal, sample efficient because surrogate guides search.

B — Model-based (MPC) + planning rollouts

Train a model that predicts next state and final reward from a state-action sequence. At decision time, run short rollouts inside the model (random/learned action sequences) to pick action maximizing predicted final 
f (MPC). Optionally combine with learned policy as warm start.

Good if function evaluations are expensive and you want to re-use experience.


Algorithms — which to use and when

TD3 / SAC: best for continuous action spaces; TD3 is deterministic-policy and often faster for deterministic envs.

PPO: robust and easy to tune; good baseline especially with discrete actions or when on-policy stability desired.

DQN / Rainbow: discrete-action problems with medium-sized action space.

Model-Based (PETS / MBPO): when evaluations are costly — you can run many “imagined” rollouts on the learned model.
